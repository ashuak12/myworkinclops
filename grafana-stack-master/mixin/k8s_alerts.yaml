namespace: kubernetes
groups:
    - name: kubernetes-apps
      rules:
        - alert: "KubePodCrashLooping"
          expr: |-
            max_over_time(kube_pod_container_status_waiting_reason{job="integrations/kubernetes/kube-state-metrics",reason="CrashLoopBackOff"}[5m]) >= 1
          for: 15m
          labels:
            severity: warning
          annotations:
            description: 'Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container }}) is in waiting state (reason: "CrashLoopBackOff").'
            summary: Pod is crash looping.
        - alert: "KubePodNotReady"
          expr: |-
            sum by (namespace, pod, cluster) (max by (namespace, pod, cluster) (kube_pod_status_phase{job="integrations/kubernetes/kube-state-metrics",phase=~"Pending|Unknown|Failed"}) * on (namespace, pod, cluster) group_left (owner_kind) topk by (namespace, pod, cluster) (1, max by (namespace, pod, owner_kind, cluster) (kube_pod_owner{owner_kind!="Job"}))) > 0
          for: 15m
          labels:
            severity: warning
          annotations:
            description: Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in a non-ready state for longer than 15 minutes.
            summary: Pod has been in a non-ready state for more than 15 minutes.
        - alert: "KubeDeploymentGenerationMismatch"
          expr: |-
            kube_deployment_status_observed_generation{job="integrations/kubernetes/kube-state-metrics"} != kube_deployment_metadata_generation{job="integrations/kubernetes/kube-state-metrics"}
          for: 15m
          labels:
            severity: warning
          annotations:
            description: Deployment generation for {{ $labels.namespace }}/{{ $labels.deployment }} does not match, this indicates that the Deployment has failed but has not been rolled back.
            summary: Deployment generation mismatch due to possible roll-back
        - alert: "KubeDeploymentReplicasMismatch"
          expr: |-
            (kube_deployment_spec_replicas{job="integrations/kubernetes/kube-state-metrics"} > kube_deployment_status_replicas_available{job="integrations/kubernetes/kube-state-metrics"}) and (changes(kube_deployment_status_replicas_updated{job="integrations/kubernetes/kube-state-metrics"}[10m]) == 0)
          for: 15m
          labels:
            severity: warning
          annotations:
            description: Deployment {{ $labels.namespace }}/{{ $labels.deployment }} has not matched the expected number of replicas for longer than 15 minutes.
            summary: Deployment has not matched the expected number of replicas.
        - alert: "KubeStatefulSetReplicasMismatch"
          expr: |-
            (kube_statefulset_status_replicas_ready{job="integrations/kubernetes/kube-state-metrics"} != kube_statefulset_status_replicas{job="integrations/kubernetes/kube-state-metrics"}) and (changes(kube_statefulset_status_replicas_updated{job="integrations/kubernetes/kube-state-metrics"}[10m]) == 0)
          for: 15m
          labels:
            severity: warning
          annotations:
            description: StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} has not matched the expected number of replicas for longer than 15 minutes.
            summary: Deployment has not matched the expected number of replicas.
        - alert: "KubeStatefulSetGenerationMismatch"
          expr: |-
            kube_statefulset_status_observed_generation{job="integrations/kubernetes/kube-state-metrics"} != kube_statefulset_metadata_generation{job="integrations/kubernetes/kube-state-metrics"}
          for: 15m
          labels:
            severity: warning
          annotations:
            description: StatefulSet generation for {{ $labels.namespace }}/{{ $labels.statefulset }} does not match, this indicates that the StatefulSet has failed but has not been rolled back.
            summary: StatefulSet generation mismatch due to possible roll-back
        - alert: "KubeStatefulSetUpdateNotRolledOut"
          expr: |-
            (max without (revision) (kube_statefulset_status_current_revision{job="integrations/kubernetes/kube-state-metrics"} unless kube_statefulset_status_update_revision{job="integrations/kubernetes/kube-state-metrics"}) * (kube_statefulset_replicas{job="integrations/kubernetes/kube-state-metrics"} != kube_statefulset_status_replicas_updated{job="integrations/kubernetes/kube-state-metrics"})) and (changes(kube_statefulset_status_replicas_updated{job="integrations/kubernetes/kube-state-metrics"}[5m]) == 0)
          for: 15m
          labels:
            severity: warning
          annotations:
            description: StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} update has not been rolled out.
            summary: StatefulSet update has not been rolled out.
        - alert: "KubeDaemonSetRolloutStuck"
          expr: |-
            ((kube_daemonset_status_current_number_scheduled{job="integrations/kubernetes/kube-state-metrics"} != kube_daemonset_status_desired_number_scheduled{job="integrations/kubernetes/kube-state-metrics"}) or (kube_daemonset_status_number_misscheduled{job="integrations/kubernetes/kube-state-metrics"} != 0) or (kube_daemonset_status_updated_number_scheduled{job="integrations/kubernetes/kube-state-metrics"} != kube_daemonset_status_desired_number_scheduled{job="integrations/kubernetes/kube-state-metrics"}) or (kube_daemonset_status_number_available{job="integrations/kubernetes/kube-state-metrics"} != kube_daemonset_status_desired_number_scheduled{job="integrations/kubernetes/kube-state-metrics"})) and (changes(kube_daemonset_status_updated_number_scheduled{job="integrations/kubernetes/kube-state-metrics"}[5m]) == 0)
          for: 15m
          labels:
            severity: warning
          annotations:
            description: DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} has not finished or progressed for at least 15 minutes.
            summary: DaemonSet rollout is stuck.
        - alert: "KubeContainerWaiting"
          expr: |-
            sum by (namespace, pod, container, cluster) (kube_pod_container_status_waiting_reason{job="integrations/kubernetes/kube-state-metrics"}) > 0
          for: 1h
          labels:
            severity: warning
          annotations:
            description: pod/{{ $labels.pod }} in namespace {{ $labels.namespace }} on container {{ $labels.container}} has been in waiting state for longer than 1 hour.
            summary: Pod container waiting longer than 1 hour
        - alert: "KubeDaemonSetNotScheduled"
          expr: |-
            kube_daemonset_status_desired_number_scheduled{job="integrations/kubernetes/kube-state-metrics"} - kube_daemonset_status_current_number_scheduled{job="integrations/kubernetes/kube-state-metrics"} > 0
          for: 10m
          labels:
            severity: warning
          annotations:
            description: '{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} are not scheduled.'
            summary: DaemonSet pods are not scheduled.
        - alert: "KubeDaemonSetMisScheduled"
          expr: |-
            kube_daemonset_status_number_misscheduled{job="integrations/kubernetes/kube-state-metrics"} > 0
          for: 15m
          labels:
            severity: warning
          annotations:
            description: '{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} are running where they are not supposed to run.'
            summary: DaemonSet pods are misscheduled.
        - alert: "KubeJobNotCompleted"
          expr: |-
            time() - max by (namespace, job_name, cluster) (kube_job_status_start_time{job="integrations/kubernetes/kube-state-metrics"} and kube_job_status_active{job="integrations/kubernetes/kube-state-metrics"} > 0) > 43200
          labels:
            severity: warning
          annotations:
            description: Job {{ $labels.namespace }}/{{ $labels.job_name }} is taking more than {{ "43200" | humanizeDuration }} to complete.
            summary: Job did not complete in time
        - alert: "KubeJobFailed"
          expr: |-
            kube_job_failed{job="integrations/kubernetes/kube-state-metrics"} > 0
          for: 15m
          labels:
            severity: warning
          annotations:
            description: Job {{ $labels.namespace }}/{{ $labels.job_name }} failed to complete. Removing failed job after investigation should clear this alert.
            summary: Job failed to complete.
        - alert: "KubeHpaReplicasMismatch"
          expr: |-
            (kube_horizontalpodautoscaler_status_desired_replicas{job="integrations/kubernetes/kube-state-metrics"} != kube_horizontalpodautoscaler_status_current_replicas{job="integrations/kubernetes/kube-state-metrics"}) and (kube_horizontalpodautoscaler_status_current_replicas{job="integrations/kubernetes/kube-state-metrics"} > kube_horizontalpodautoscaler_spec_min_replicas{job="integrations/kubernetes/kube-state-metrics"}) and (kube_horizontalpodautoscaler_status_current_replicas{job="integrations/kubernetes/kube-state-metrics"} < kube_horizontalpodautoscaler_spec_max_replicas{job="integrations/kubernetes/kube-state-metrics"}) and changes(kube_horizontalpodautoscaler_status_current_replicas{job="integrations/kubernetes/kube-state-metrics"}[15m]) == 0
          for: 15m
          labels:
            severity: warning
          annotations:
            description: HPA {{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler  }} has not matched the desired number of replicas for longer than 15 minutes.
            summary: HPA has not matched desired number of replicas.
        - alert: "KubeHpaMaxedOut"
          expr: |-
            kube_horizontalpodautoscaler_status_current_replicas{job="integrations/kubernetes/kube-state-metrics"} == kube_horizontalpodautoscaler_spec_max_replicas{job="integrations/kubernetes/kube-state-metrics"}
          for: 15m
          labels:
            severity: warning
          annotations:
            description: HPA {{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler  }} has been running at max replicas for longer than 15 minutes.
            summary: HPA is running at max replicas
    - name: kubernetes-resources
      rules:
        - alert: "KubeCPUOvercommit"
          expr: |-
            sum(namespace_cpu:kube_pod_container_resource_requests:sum) - (sum(kube_node_status_allocatable{resource="cpu"}) - max(kube_node_status_allocatable{resource="cpu"})) > 0 and (sum(kube_node_status_allocatable{resource="cpu"}) - max(kube_node_status_allocatable{resource="cpu"})) > 0
          for: 10m
          labels:
            severity: warning
          annotations:
            description: Cluster has overcommitted CPU resource requests for Pods by {{ $value }} CPU shares and cannot tolerate node failure.
            summary: Cluster has overcommitted CPU resource requests.
        - alert: "KubeMemoryOvercommit"
          expr: |-
            sum(namespace_memory:kube_pod_container_resource_requests:sum) - (sum(kube_node_status_allocatable{resource="memory"}) - max(kube_node_status_allocatable{resource="memory"})) > 0 and (sum(kube_node_status_allocatable{resource="memory"}) - max(kube_node_status_allocatable{resource="memory"})) > 0
          for: 10m
          labels:
            severity: warning
          annotations:
            description: Cluster has overcommitted memory resource requests for Pods by {{ $value | humanize }} bytes and cannot tolerate node failure.
            summary: Cluster has overcommitted memory resource requests.
        - alert: "KubeCPUQuotaOvercommit"
          expr: |-
            sum(min without (resource) (kube_resourcequota{job="integrations/kubernetes/kube-state-metrics",resource=~"(cpu|requests.cpu)",type="hard"})) / sum(kube_node_status_allocatable{job="integrations/kubernetes/kube-state-metrics",resource="cpu"}) > 1.5
          for: 5m
          labels:
            severity: warning
          annotations:
            description: Cluster has overcommitted CPU resource requests for Namespaces.
            summary: Cluster has overcommitted CPU resource requests.
        - alert: "KubeMemoryQuotaOvercommit"
          expr: |-
            sum(min without (resource) (kube_resourcequota{job="integrations/kubernetes/kube-state-metrics",resource=~"(memory|requests.memory)",type="hard"})) / sum(kube_node_status_allocatable{job="integrations/kubernetes/kube-state-metrics",resource="memory"}) > 1.5
          for: 5m
          labels:
            severity: warning
          annotations:
            description: Cluster has overcommitted memory resource requests for Namespaces.
            summary: Cluster has overcommitted memory resource requests.
        - alert: "KubeQuotaAlmostFull"
          expr: |-
            kube_resourcequota{job="integrations/kubernetes/kube-state-metrics",type="used"} / ignoring (instance, job, type) (kube_resourcequota{job="integrations/kubernetes/kube-state-metrics",type="hard"} > 0) > 0.9 < 1
          for: 15m
          labels:
            severity: info
          annotations:
            description: Namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage }} of its {{ $labels.resource }} quota.
            summary: Namespace quota is going to be full.
        - alert: "KubeQuotaFullyUsed"
          expr: |-
            kube_resourcequota{job="integrations/kubernetes/kube-state-metrics",type="used"} / ignoring (instance, job, type) (kube_resourcequota{job="integrations/kubernetes/kube-state-metrics",type="hard"} > 0) == 1
          for: 15m
          labels:
            severity: info
          annotations:
            description: Namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage }} of its {{ $labels.resource }} quota.
            summary: Namespace quota is fully used.
        - alert: "KubeQuotaExceeded"
          expr: |-
            kube_resourcequota{job="integrations/kubernetes/kube-state-metrics",type="used"} / ignoring (instance, job, type) (kube_resourcequota{job="integrations/kubernetes/kube-state-metrics",type="hard"} > 0) > 1
          for: 15m
          labels:
            severity: warning
          annotations:
            description: Namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage }} of its {{ $labels.resource }} quota.
            summary: Namespace quota has exceeded the limits.
        - alert: "CPUThrottlingHigh"
          expr: |-
            sum by (container, pod, namespace) (increase(container_cpu_cfs_throttled_periods_total{container!=""}[5m])) / sum by (container, pod, namespace) (increase(container_cpu_cfs_periods_total[5m])) > (25 / 100)
          for: 15m
          labels:
            severity: info
          annotations:
            description: '{{ $value | humanizePercentage }} throttling of CPU in namespace {{ $labels.namespace }} for container {{ $labels.container }} in pod {{ $labels.pod }}.'
            summary: Processes experience elevated CPU throttling.
    - name: kubernetes-storage
      rules:
        - alert: "KubePersistentVolumeFillingUp"
          expr: |-
            (kubelet_volume_stats_available_bytes{job="integrations/kubernetes/kubelet"} / kubelet_volume_stats_capacity_bytes{job="integrations/kubernetes/kubelet"}) < 0.03 and kubelet_volume_stats_used_bytes{job="integrations/kubernetes/kubelet"} > 0 unless on (namespace, persistentvolumeclaim) kube_persistentvolumeclaim_access_mode{access_mode="ReadOnlyMany"} == 1 unless on (namespace, persistentvolumeclaim) kube_persistentvolumeclaim_labels{label_excluded_from_alerts="true"} == 1
          for: 1m
          labels:
            severity: critical
          annotations:
            description: The PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} is only {{ $value | humanizePercentage }} free.
            summary: PersistentVolume is filling up.
        - alert: "KubePersistentVolumeFillingUp"
          expr: |-
            (kubelet_volume_stats_available_bytes{job="integrations/kubernetes/kubelet"} / kubelet_volume_stats_capacity_bytes{job="integrations/kubernetes/kubelet"}) < 0.15 and kubelet_volume_stats_used_bytes{job="integrations/kubernetes/kubelet"} > 0 and predict_linear(kubelet_volume_stats_available_bytes{job="integrations/kubernetes/kubelet"}[6h], 4 * 24 * 3600) < 0 unless on (namespace, persistentvolumeclaim) kube_persistentvolumeclaim_access_mode{access_mode="ReadOnlyMany"} == 1 unless on (namespace, persistentvolumeclaim) kube_persistentvolumeclaim_labels{label_excluded_from_alerts="true"} == 1
          for: 1h
          labels:
            severity: warning
          annotations:
            description: Based on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} is expected to fill up within four days. Currently {{ $value | humanizePercentage }} is available.
            summary: PersistentVolume is filling up.
        - alert: "KubePersistentVolumeInodesFillingUp"
          expr: |-
            (kubelet_volume_stats_inodes_free{job="integrations/kubernetes/kubelet"} / kubelet_volume_stats_inodes{job="integrations/kubernetes/kubelet"}) < 0.03 and kubelet_volume_stats_inodes_used{job="integrations/kubernetes/kubelet"} > 0 unless on (namespace, persistentvolumeclaim) kube_persistentvolumeclaim_access_mode{access_mode="ReadOnlyMany"} == 1 unless on (namespace, persistentvolumeclaim) kube_persistentvolumeclaim_labels{label_excluded_from_alerts="true"} == 1
          for: 1m
          labels:
            severity: critical
          annotations:
            description: The PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} only has {{ $value | humanizePercentage }} free inodes.
            summary: PersistentVolumeInodes are filling up.
        - alert: "KubePersistentVolumeInodesFillingUp"
          expr: |-
            (kubelet_volume_stats_inodes_free{job="integrations/kubernetes/kubelet"} / kubelet_volume_stats_inodes{job="integrations/kubernetes/kubelet"}) < 0.15 and kubelet_volume_stats_inodes_used{job="integrations/kubernetes/kubelet"} > 0 and predict_linear(kubelet_volume_stats_inodes_free{job="integrations/kubernetes/kubelet"}[6h], 4 * 24 * 3600) < 0 unless on (namespace, persistentvolumeclaim) kube_persistentvolumeclaim_access_mode{access_mode="ReadOnlyMany"} == 1 unless on (namespace, persistentvolumeclaim) kube_persistentvolumeclaim_labels{label_excluded_from_alerts="true"} == 1
          for: 1h
          labels:
            severity: warning
          annotations:
            description: Based on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} is expected to run out of inodes within four days. Currently {{ $value | humanizePercentage }} of its inodes are free.
            summary: PersistentVolumeInodes are filling up.
        - alert: "KubePersistentVolumeErrors"
          expr: |-
            kube_persistentvolume_status_phase{job="integrations/kubernetes/kube-state-metrics",phase=~"Failed|Pending"} > 0
          for: 5m
          labels:
            severity: critical
          annotations:
            description: The persistent volume {{ $labels.persistentvolume }} has status {{ $labels.phase }}.
            summary: PersistentVolume is having issues with provisioning.
    - name: kubernetes-system-kubelet
      rules:
        - alert: "KubeNodeNotReady"
          expr: |-
            kube_node_status_condition{condition="Ready",job="integrations/kubernetes/kube-state-metrics",status="true"} == 0
          for: 15m
          labels:
            severity: warning
          annotations:
            description: '{{ $labels.node }} has been unready for more than 15 minutes.'
            summary: Node is not ready.
        - alert: "KubeNodeUnreachable"
          expr: |-
            (kube_node_spec_taint{effect="NoSchedule",job="integrations/kubernetes/kube-state-metrics",key="node.kubernetes.io/unreachable"} unless ignoring (key, value) kube_node_spec_taint{job="integrations/kubernetes/kube-state-metrics",key=~"ToBeDeletedByClusterAutoscaler|cloud.google.com/impending-node-termination|aws-node-termination-handler/spot-itn"}) == 1
          for: 15m
          labels:
            severity: warning
          annotations:
            description: '{{ $labels.node }} is unreachable and some workloads may be rescheduled.'
            summary: Node is unreachable.
        - alert: "KubeletTooManyPods"
          expr: |-
            count by (cluster, node) ((kube_pod_status_phase{job="integrations/kubernetes/kube-state-metrics",phase="Running"} == 1) * on (instance, pod, namespace, cluster) group_left (node) topk by (instance, pod, namespace, cluster) (1, kube_pod_info{job="integrations/kubernetes/kube-state-metrics"})) / max by (cluster, node) (kube_node_status_capacity{job="integrations/kubernetes/kube-state-metrics",resource="pods"} != 1) > 0.95
          for: 15m
          labels:
            severity: info
          annotations:
            description: Kubelet '{{ $labels.node }}' is running at {{ $value | humanizePercentage }} of its Pod capacity.
            summary: Kubelet is running at capacity.
        - alert: "KubeNodeReadinessFlapping"
          expr: |-
            sum by (cluster, node) (changes(kube_node_status_condition{condition="Ready",status="true"}[15m])) > 2
          for: 15m
          labels:
            severity: warning
          annotations:
            description: The readiness status of node {{ $labels.node }} has changed {{ $value }} times in the last 15 minutes.
            summary: Node readiness status is flapping.
        - alert: "KubeletPlegDurationHigh"
          expr: |-
            node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile{quantile="0.99"} >= 10
          for: 5m
          labels:
            severity: warning
          annotations:
            description: The Kubelet Pod Lifecycle Event Generator has a 99th percentile duration of {{ $value }} seconds on node {{ $labels.node }}.
            summary: Kubelet Pod Lifecycle Event Generator is taking too long to relist.
        - alert: "KubeletPodStartUpLatencyHigh"
          expr: |-
            histogram_quantile(0.99, sum by (cluster, instance, le) (rate(kubelet_pod_worker_duration_seconds_bucket{job="integrations/kubernetes/kubelet"}[5m]))) * on (cluster, instance) group_left (node) kubelet_node_name{job="integrations/kubernetes/kubelet"} > 60
          for: 15m
          labels:
            severity: warning
          annotations:
            description: Kubelet Pod startup 99th percentile latency is {{ $value }} seconds on node {{ $labels.node }}.
            summary: Kubelet Pod startup latency is too high.
        - alert: "KubeletClientCertificateExpiration"
          expr: |-
            kubelet_certificate_manager_client_ttl_seconds < 604800
          labels:
            severity: warning
          annotations:
            description: Client certificate for Kubelet on node {{ $labels.node }} expires in {{ $value | humanizeDuration }}.
            summary: Kubelet client certificate is about to expire.
        - alert: "KubeletClientCertificateExpiration"
          expr: |-
            kubelet_certificate_manager_client_ttl_seconds < 86400
          labels:
            severity: critical
          annotations:
            description: Client certificate for Kubelet on node {{ $labels.node }} expires in {{ $value | humanizeDuration }}.
            summary: Kubelet client certificate is about to expire.
        - alert: "KubeletServerCertificateExpiration"
          expr: |-
            kubelet_certificate_manager_server_ttl_seconds < 604800
          labels:
            severity: warning
          annotations:
            description: Server certificate for Kubelet on node {{ $labels.node }} expires in {{ $value | humanizeDuration }}.
            summary: Kubelet server certificate is about to expire.
        - alert: "KubeletServerCertificateExpiration"
          expr: |-
            kubelet_certificate_manager_server_ttl_seconds < 86400
          labels:
            severity: critical
          annotations:
            description: Server certificate for Kubelet on node {{ $labels.node }} expires in {{ $value | humanizeDuration }}.
            summary: Kubelet server certificate is about to expire.
        - alert: "KubeletClientCertificateRenewalErrors"
          expr: |-
            increase(kubelet_certificate_manager_client_expiration_renew_errors[5m]) > 0
          for: 15m
          labels:
            severity: warning
          annotations:
            description: Kubelet on node {{ $labels.node }} has failed to renew its client certificate ({{ $value | humanize }} errors in the last 5 minutes).
            summary: Kubelet has failed to renew its client certificate.
        - alert: "KubeletServerCertificateRenewalErrors"
          expr: |-
            increase(kubelet_server_expiration_renew_errors[5m]) > 0
          for: 15m
          labels:
            severity: warning
          annotations:
            description: Kubelet on node {{ $labels.node }} has failed to renew its server certificate ({{ $value | humanize }} errors in the last 5 minutes).
            summary: Kubelet has failed to renew its server certificate.
        - alert: "KubeletDown"
          expr: |-
            absent(up{job="integrations/kubernetes/kubelet"} == 1)
          for: 15m
          labels:
            severity: critical
          annotations:
            description: Kubelet has disappeared from Prometheus target discovery.
            summary: Target disappeared from Prometheus target discovery.
